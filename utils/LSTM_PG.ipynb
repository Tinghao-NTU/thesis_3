{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "welcome-mailman",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "import torch.optim as optim\n",
    "from copy import deepcopy\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "#from tensorboardX import SummaryWriter\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self,n_states, n_hidden, n_output,num_layers = 2):\n",
    "        super(Policy, self).__init__()\n",
    "        self.LSTM = nn.LSTM(n_states, n_hidden,num_layers)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.num_layers = num_layers\n",
    "        self.linear = nn.Linear(n_hidden, n_output)\n",
    "    def forward(self, input):\n",
    "        hns = torch.zeros(self.num_layers,1,self.n_hidden)\n",
    "        cns = torch.zeros(self.num_layers,1,self.n_hidden)\n",
    "        outputs = []\n",
    "        for i in range(len(input)):\n",
    "            x = input[i,:].view(1,1,-1)\n",
    "            output, (hns, cns) = self.LSTM(x, (hns, cns))\n",
    "            output = F.softmax(self.linear(output), dim= 2)\n",
    "            outputs.append(output)\n",
    "        return torch.cat(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "popular-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Policy(80,100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "explicit-aging",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4909, 0.5091]],\n",
       "\n",
       "        [[0.4935, 0.5065]],\n",
       "\n",
       "        [[0.4964, 0.5036]],\n",
       "\n",
       "        [[0.4978, 0.5022]],\n",
       "\n",
       "        [[0.4992, 0.5008]],\n",
       "\n",
       "        [[0.5000, 0.5000]],\n",
       "\n",
       "        [[0.5019, 0.4981]],\n",
       "\n",
       "        [[0.5027, 0.4973]],\n",
       "\n",
       "        [[0.5046, 0.4954]],\n",
       "\n",
       "        [[0.5050, 0.4950]],\n",
       "\n",
       "        [[0.5052, 0.4948]],\n",
       "\n",
       "        [[0.5044, 0.4956]],\n",
       "\n",
       "        [[0.5042, 0.4958]],\n",
       "\n",
       "        [[0.5026, 0.4974]],\n",
       "\n",
       "        [[0.5015, 0.4985]],\n",
       "\n",
       "        [[0.5035, 0.4965]],\n",
       "\n",
       "        [[0.5039, 0.4961]],\n",
       "\n",
       "        [[0.5032, 0.4968]],\n",
       "\n",
       "        [[0.5053, 0.4947]],\n",
       "\n",
       "        [[0.5055, 0.4945]],\n",
       "\n",
       "        [[0.5057, 0.4943]],\n",
       "\n",
       "        [[0.5051, 0.4949]],\n",
       "\n",
       "        [[0.5043, 0.4957]],\n",
       "\n",
       "        [[0.5041, 0.4959]],\n",
       "\n",
       "        [[0.5045, 0.4955]],\n",
       "\n",
       "        [[0.5042, 0.4958]],\n",
       "\n",
       "        [[0.5059, 0.4941]],\n",
       "\n",
       "        [[0.5040, 0.4960]],\n",
       "\n",
       "        [[0.5039, 0.4961]],\n",
       "\n",
       "        [[0.5051, 0.4949]],\n",
       "\n",
       "        [[0.5046, 0.4954]],\n",
       "\n",
       "        [[0.5050, 0.4950]],\n",
       "\n",
       "        [[0.5045, 0.4955]],\n",
       "\n",
       "        [[0.5026, 0.4974]],\n",
       "\n",
       "        [[0.5039, 0.4961]],\n",
       "\n",
       "        [[0.5051, 0.4949]],\n",
       "\n",
       "        [[0.5052, 0.4948]],\n",
       "\n",
       "        [[0.5044, 0.4956]],\n",
       "\n",
       "        [[0.5058, 0.4942]],\n",
       "\n",
       "        [[0.5063, 0.4937]],\n",
       "\n",
       "        [[0.5067, 0.4933]],\n",
       "\n",
       "        [[0.5068, 0.4932]],\n",
       "\n",
       "        [[0.5072, 0.4928]],\n",
       "\n",
       "        [[0.5071, 0.4929]],\n",
       "\n",
       "        [[0.5049, 0.4951]],\n",
       "\n",
       "        [[0.5025, 0.4975]],\n",
       "\n",
       "        [[0.5009, 0.4991]],\n",
       "\n",
       "        [[0.5017, 0.4983]],\n",
       "\n",
       "        [[0.5016, 0.4984]],\n",
       "\n",
       "        [[0.5036, 0.4964]],\n",
       "\n",
       "        [[0.5028, 0.4972]],\n",
       "\n",
       "        [[0.5039, 0.4961]],\n",
       "\n",
       "        [[0.5041, 0.4959]],\n",
       "\n",
       "        [[0.5034, 0.4966]],\n",
       "\n",
       "        [[0.5037, 0.4963]],\n",
       "\n",
       "        [[0.5052, 0.4948]],\n",
       "\n",
       "        [[0.5054, 0.4946]],\n",
       "\n",
       "        [[0.5050, 0.4950]],\n",
       "\n",
       "        [[0.5043, 0.4957]],\n",
       "\n",
       "        [[0.5034, 0.4966]],\n",
       "\n",
       "        [[0.5019, 0.4981]],\n",
       "\n",
       "        [[0.5032, 0.4968]],\n",
       "\n",
       "        [[0.5042, 0.4958]],\n",
       "\n",
       "        [[0.5048, 0.4952]],\n",
       "\n",
       "        [[0.5046, 0.4954]],\n",
       "\n",
       "        [[0.5055, 0.4945]],\n",
       "\n",
       "        [[0.5051, 0.4949]],\n",
       "\n",
       "        [[0.5055, 0.4945]],\n",
       "\n",
       "        [[0.5068, 0.4932]],\n",
       "\n",
       "        [[0.5069, 0.4931]],\n",
       "\n",
       "        [[0.5059, 0.4941]],\n",
       "\n",
       "        [[0.5058, 0.4942]],\n",
       "\n",
       "        [[0.5052, 0.4948]],\n",
       "\n",
       "        [[0.5034, 0.4966]],\n",
       "\n",
       "        [[0.5026, 0.4974]],\n",
       "\n",
       "        [[0.5039, 0.4961]],\n",
       "\n",
       "        [[0.5032, 0.4968]],\n",
       "\n",
       "        [[0.5041, 0.4959]],\n",
       "\n",
       "        [[0.5060, 0.4940]],\n",
       "\n",
       "        [[0.5048, 0.4952]],\n",
       "\n",
       "        [[0.5039, 0.4961]],\n",
       "\n",
       "        [[0.5025, 0.4975]],\n",
       "\n",
       "        [[0.5032, 0.4968]],\n",
       "\n",
       "        [[0.5039, 0.4961]],\n",
       "\n",
       "        [[0.5059, 0.4941]],\n",
       "\n",
       "        [[0.5050, 0.4950]],\n",
       "\n",
       "        [[0.5051, 0.4949]],\n",
       "\n",
       "        [[0.5067, 0.4933]],\n",
       "\n",
       "        [[0.5057, 0.4943]],\n",
       "\n",
       "        [[0.5056, 0.4944]],\n",
       "\n",
       "        [[0.5026, 0.4974]],\n",
       "\n",
       "        [[0.5018, 0.4982]],\n",
       "\n",
       "        [[0.5023, 0.4977]],\n",
       "\n",
       "        [[0.5017, 0.4983]],\n",
       "\n",
       "        [[0.5018, 0.4982]],\n",
       "\n",
       "        [[0.5011, 0.4989]],\n",
       "\n",
       "        [[0.5022, 0.4978]],\n",
       "\n",
       "        [[0.5011, 0.4989]],\n",
       "\n",
       "        [[0.5021, 0.4979]],\n",
       "\n",
       "        [[0.5020, 0.4980]]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.rand(100,80)\n",
    "a(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "streaming-pepper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1820, 0.3450, 0.1748, 0.0449, 0.0508, 0.7228, 0.6811, 0.0889, 0.0966,\n",
       "        0.5721, 0.6930, 0.5264, 0.8829, 0.4241, 0.6255, 0.3674, 0.6330, 0.0537,\n",
       "        0.3037, 0.7615, 0.0547, 0.3017, 0.0513, 0.0655, 0.6392, 0.0936, 0.8664,\n",
       "        0.4559, 0.4171, 0.8298, 0.1635, 0.8845, 0.3580, 0.3137, 0.8301, 0.3508,\n",
       "        0.4007, 0.0387, 0.0030, 0.6286, 0.2429, 0.0224, 0.8250, 0.1202, 0.7630,\n",
       "        0.5056, 0.8896, 0.5973, 0.8189, 0.7630, 0.0769, 0.7675, 0.9364, 0.7101,\n",
       "        0.0400, 0.2877, 0.0953, 0.0618, 0.7114, 0.9062, 0.2458, 0.0229, 0.7927,\n",
       "        0.2239, 0.8524, 0.9350, 0.1572, 0.3081, 0.4036, 0.8021, 0.6250, 0.5991,\n",
       "        0.6583, 0.3102, 0.5815, 0.4139, 0.8491, 0.7776, 0.3083, 0.2211])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-firewall",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
